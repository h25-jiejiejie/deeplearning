神经网络逻辑回归可以看成Z=W.T*X+B

其中W是很多个列向量w组成，在每个隐藏层单元每个w都是自己

每个隐藏层的隐藏单元分成两步：1）计算Z。2）计算激活后的a(simo/RELU啥的)

每一隐藏层的隐藏单元的w的维度都是由上一层的输出决定的

sigmoid事实上比较差，但是如果y∈{0，1}，那sigmoid函数会好（一般在就希望y^大于0小于1的时候使用）

