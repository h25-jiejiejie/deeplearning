$神经网络逻辑回归可以看成Z=W.T*X+B
 激活：a=g(Z)---------------a和A

其中W是很多个列向量w组成，在每个隐藏层单元每个w都是自己

每个隐藏层的隐藏单元分成两步：1）计算Z。2）计算激活后的a(simo/RELU啥的)

每一隐藏层的隐藏单元的w的维度都是由上一层的输出决定的

sigmoid事实上比较差，但是如果y∈{0，1}，那sigmoid函数会好（一般在就希望y^大于0小于1的时候使用），一般不知道使用什么作为激活函数时选择RELU

神经网络训练的初始的时候随机初始化权重很重要

参数更新的公式（用w来举例）：w=w-αdw     (其中α表示学习率，dw是梯度下降的倒数)-------------反向传播时进行

```python
np.sum(x,axis=0,keepdims=true)
#其中x就是需要求和的矩阵，axis=0表示竖直方向上求和（=1是垂直方向），keepdims表示保留这个样子，不会讲矩阵输出生为数组形式
#如果np.shape输出为（n, ）这种形式表示这是个数组不是矩阵
```

```python
A = np.zero(x,y) #初始化A矩阵全部是0
B = np.one（x,y）#初始化B矩阵全部是1
A = A.reshape(x1,y1) #重塑矩阵的状态，注意的是重塑的矩阵的元素数量不变才行
A.T #这个是矩阵的转置
```

如果不将权重随机初始化一个小值，一个层中的所有单元都会做一样的事情，这样就没有意义了

并且随机初始化W的时候需要一个小权重就行这个0.01只是一个例子；因为如果随机初始化权重很大的话在做像是二分类这样的问题的时候过激活函数Z也就是a会很大，他的导数就会接近0，遮藏梯度下降就会很慢（RELU不会有这种问题）
```python
W=np.random.randn(x,y)*0.01 #随机初始话权重W，很小的随机数
B=np.zeros(x,y) #B无所谓，不会影响他的平衡性
```



$向前传播（神经网络前进）：得到各个输出，缓存用于反向传播时候的权重更新（导数计算这些做参数更新）

神经元参数更新步骤：在正向传播过程中进行正向传播的函数后缓存值，而后在反向传播中进行反向传播的函数进行权重的更新

$各个参数更新的导函数：

sigmoid:ds=s * (1 - s)-------其中s表示激活函数后的数g(Z)
dw=(1/m)*X((A-Y).T)
db=(1/m)*sum(a[i]-y[i])

注：X是这一层的输入；A=g(Z)；Y是目标输出值（与Yhat区别，Yhat其实算是A）

$归一化行：

首先将需要的行或者整个矩阵进行模运算，模运算前为x，模运算后是||x||；之后进行元素/模运算（x/||x||）；这就是归一化

```python
def normalizeRows(x):
    x_norm=np.linalg.norm(x,axis=1,keepdims=True)
    x=x/x_norm
    return x
```

$softmax函数：可以将这个函数看作一个归一化?
```python
def softmax(x):
    x_exp = np.exp(x)
    x_sum= np.sum(x_exp,axis=1,keepdims=True)
    s = x_exp/x_sum
    return s 
```

$np的一些用法：

内积dot：内积就像是矩阵乘法（点乘）

外积np.outer(x,y)：就是两个数组的外积，当两个是矩阵的时候会自动将他们扁平化，就是x[i]*y[j]，输出是一个矩阵。第一行是x第一个元素和y所有元素成绩组成的行向量。

逐项元素相乘np.multiply(A,B)：就是将两个对应的元素进行相乘输出同样维度的矩阵；直接*也可以（相当于）

幂运算np.power(x,num)：左边是一个值，右边是幂函数的数，还有就是直接**num也是幂函数运算

np.squeeze(x,axis)，用于消除维度为1的矩阵，也就是如果没有axis指定删除，就会将(1,3,1,1)->(3,)，指定只会删除指定的维度上是1的部分axis=0,->(3,1,1)

$参数和超参数（hyperparameters）:

超参数：像是隐藏层（L），隐藏层的隐藏单元个数（n），学习率（α），迭代次数，激活函数等等会影响W和b的；动量，批大小，各种形式的正则化

参数：W,b

Z[L]=W[L]X+b[L]

A[L]=g[L](Z[L])--------------最后一层A=Yhat

dZ[L]=A[L]-Y

dW[L]=(1/m)X(dZ[L].T)        --其中X是输入这一层的数据，也是就是上一层输出的A

db[L]=(1/m)np.sum(dZ[L])     ----由于只是一层隐藏层所以没有,axis=1,keepdims=True，如果要求输出的隐藏层大于1就要加，因为这是因为每个神经元的b不一样



```python
注意：可以是多维度的，分成输入层数和输出隐藏层，输入就是上一层A的向量大小，隐藏层数就是神经元的多少，其中每个神经元的w都会与输入进行计算输出的是1个数，不是一个一个神经元这样的构造只是状态如此
```
