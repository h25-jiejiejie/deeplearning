$神经网络逻辑回归可以看成Z=W.T*X+B
 激活：a=g(Z)---------------a和A

其中W是很多个列向量w组成，在每个隐藏层单元每个w都是自己

每个隐藏层的隐藏单元分成两步：1）计算Z。2）计算激活后的a(simo/RELU啥的)

每一隐藏层的隐藏单元的w的维度都是由上一层的输出决定的

sigmoid事实上比较差，但是如果y∈{0，1}，那sigmoid函数会好（一般在就希望y^大于0小于1的时候使用），一般不知道使用什么作为激活函数时选择RELU

神经网络训练的初始的时候随机初始化权重很重要

参数更新的公式（用w来举例）：w=w-αdw     (其中α表示学习率，dw是梯度下降的倒数)-------------反向传播时进行

```python
np.sum(x,axis=0,keepdims=true)
#其中x就是需要求和的矩阵，axis=0表示竖直方向上求和（=1是垂直方向），keepdims表示保留这个样子，不会讲矩阵输出生为数组形式
#如果np.shape输出为（n, ）这种形式表示这是个数组不是矩阵
```

```python
A = np.zero(x,y) #初始化A矩阵全部是0
B = np.one（x,y）#初始化B矩阵全部是1
A = A.reshape(x1,y1) #重塑矩阵的状态，注意的是重塑的矩阵的元素数量不变才行
A.T #这个是矩阵的转置
```

如果不将权重随机初始化一个小值，一个层中的所有单元都会做一样的事情，这样就没有意义了

并且随机初始化W的时候需要一个小权重就行这个0.01只是一个例子；因为如果随机初始化权重很大的话在做像是二分类这样的问题的时候过激活函数Z也就是a会很大，他的导数就会接近0，遮藏梯度下降就会很慢（RELU不会有这种问题）
```python
W=np.random.randn(x,y)*0.01 #随机初始话权重W，很小的随机数
B=np.zeros(x,y) #B无所谓，不会影响他的平衡性
```



$向前传播（神经网络前进）：得到各个输出，缓存用于反向传播时候的权重更新（导数计算这些做参数更新）

神经元参数更新步骤：在正向传播过程中进行正向传播的函数后缓存值，而后在反向传播中进行反向传播的函数进行权重的更新

$各个参数更新的导函数：

sigmoid:ds=s * (1 - s)-------其中s表示激活函数后的数g(Z)

$归一化行：

首先将需要的行或者整个矩阵进行模运算，模运算前为x，模运算后是||x||；之后进行元素/模运算（x/||x||）；这就是归一化

```python
def normalizeRows(x):
    x_norm=np.linalg.norm(x,axis=1,keepdims=True)
    x=x/x_norm
    return x
```
