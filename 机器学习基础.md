$高方差和高偏差（两种情况会出现同时出现的情况）

高方差（high variance）:训练集拟合很好，但是验证集（dev）差的情况（验证集比训练集差很多）

高偏差（high bias）：模型对训练集欠拟合的情况

解决方法：（选择一种更合适的网络架构可能直接全部解决）

高偏差：一个新的网络或者更大的网络（*）和隐藏层，一些方法，至少可以拟合训练集再说。

高方差：更多数据（最高方法*），正则化方法

$正则化（regularization）：现在更多趋向于L2正则化

L2正则化：J(w,b)=(1/m)*sum(ɻ(yhat,y))+(人/2m)||w||^2---------------------------------这只是一个参数的例子，可以加b啥的。其中||w||=w.T*w，其中（人）是正则化参数（属于超参数），在代码中由于lambda是一个保留字写成lambd就行了

L1:将||w||^2转成||w||

例子：dw更新成：dw=(原先)+(λ/m)*w

正则化减少过拟合：解释是λ大的化，W就会减少，Z就会被惩罚导致Z减少，使得激活函数在线性范围内。

$dropout正则化：

随机丢弃一些单元，将他的激活后输出变成0，从而在反向传播中不会进行梯度更新，参数不会改变，不是参数（像是w）变成0，只是不参与反向传播的梯度更新

```python
def dropout_m(X,keep_prob):
    dropout_mask = np.random.rand(X.shape[0],X.shape[1]) < keep_prob        #生成随机向量，当小于保留概率的时候就是0
    a=np.multiply(X,dropout_mask)                                           #将单元进行丢弃
    a/= keep_prob                                                           #缩放回原来的大小，本质这个a其实就是输入X的dropout版本
    return a
```
