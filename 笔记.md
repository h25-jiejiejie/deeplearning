神经网络逻辑回归可以看成Z=W.T*X+B

其中W是很多个列向量w组成，在每个隐藏层单元每个w都是自己

每个隐藏层的隐藏单元分成两步：1）计算Z。2）计算激活后的a(simo/RELU啥的)

每一隐藏层的隐藏单元的w的维度都是由上一层的输出决定的

sigmoid事实上比较差，但是如果y∈{0，1}，那sigmoid函数会好（一般在就希望y^大于0小于1的时候使用），一般不知道使用什么作为激活函数时选择RELU

神经网络训练的初始的时候随机初始化参数很重要

参数更新的公式（用w来举例）：w=w-αdw     (其中α表示学习率，dw是梯度下降的倒数)

```python
np.sum(x,axis=0,keepdims=true)
#其中x就是需要求和的矩阵，axis=0表示竖直方向上求和（=1是垂直方向），keepdims表示保留这个样子，不会讲矩阵输出生为数组形式
#如果np,shape输出为（n, ）这种形式表示这是个数组不是矩阵
```

```python
A = np.zero(x,y) #初始化A矩阵全部是0
B = np.one（x,y）#初始化B矩阵全部是1
A.reshape(x1,y1) #重塑矩阵的状态，注意的是重塑的矩阵的元素数量不变才行
A.T #这个是矩阵的转置
```

