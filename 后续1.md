```python
调试处理：
  超参数重要性：
    最重要：学习率
    第二重要：动量β，批次大小，隐藏单元数，
    第三重要：隐藏层数，学习率递减

  超参数空间的选择：
    超参量较小的时候：不要在网格中进行比对，建议是随机选择点，在随机选择的点上进行超参数的尝试（现在假设只有两个超参数形成的二维图形中，要随机选择点）
    一般选择（由粗到细）：在整个范围内随机选择，在这些随机好的里面相近的点的范围在随机选择
```

```python
超参数选择合适的范围：
  1.学习率；选择的时候先确定一个范围（比如在0.0001和1之间是我们认为合理的选择），之后生成使用
  α=-4*np.random.rand()     #这里是-4因为0.0001是因为log10(0.0001)=-4，得到的数就会在选择的学习率之间，rand()是均匀分布生成0-1的float

  2.动量或者SRM的β：由于推荐在0.9或者0.9999这种，在（1-β）上进行随机采样，于是就和上面的学习率的选择相同，花费更小的代价
  a=-4*np.random.rand()
  β=1-a
```

```python
实践：（在不同的领域，超参数的选择方式有些不同，但是相互借鉴）隔一段时间重新评估一下超参数
  流派1（熊猫法）：照看，比如每天调整学习率和其他超参数在查看表现，上下调整超参数
  流派2（鱼子酱法）：训练很多模型，查看那个好用
```

```python
正则化网络的激活函数：
  归一化z[2]更多（默认）

  批量归一化batch norm（bn）：
  mean=1/m*np.sum(Z)                          #标准化
  var=1/m*((Z-mean)**2)                       #正则化步骤，计算方差
  Znorm=(Z-mean)/(np.sqrt(var+epsilon))       #epsilon方式除以0的参数，归一化训练稳定加速收敛，但是将输入分布标准到正太分布中（均值0方差1）会对神经网络本身的表达能力限制
  Z_tilde = gamma * Znorm + beta             # 重参数化，最终输出，表达能力提升（对上面代码的逆向）
```

```python
将批量归一化拟合进神经网络：
  一般与小批量数据进行使用，将原来的Z批量化成Ztilde，再计算A
  在批量归一化中会将b进行平均删除，也就是说b其实相当于没有了

  步骤：
    网络向前传播，使用Ztilde来替换Z，计算A后；
    反向传播，计算所有层的dw,db,dbeta,dgamma，使用学习率更新参数
```

```python
批量归一化为什么好用：
  由于将输入特征先归一化成均值为0，方差为1的特征，他们具有相同的取值范围，加速学习。

  批量归一化的作用是将网络层数的参数A这种偏不大，即使在更新但是依旧分布在相同分布中，变化较小，这样每个层数就会比较独立（早期层不能改变太多这样后期层会更容易的学习）

  由于在小批量上实现，所以会引入噪声，但是结果类似DROPOUT，有轻微正则化的意义（但是不要当作相当于）
```

```python
测试时候的bach norm：
  
```

```python

```













