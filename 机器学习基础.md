$高方差和高偏差（两种情况会出现同时出现的情况）

高方差（high variance）:训练集拟合很好，但是验证集（dev）差的情况（验证集比训练集差很多）

高偏差（high bias）：模型对训练集欠拟合的情况

解决方法：（选择一种更合适的网络架构可能直接全部解决）

高偏差：一个新的网络或者更大的网络（*）和隐藏层，一些方法，至少可以拟合训练集再说。

高方差：更多数据（最高方法*），正则化方法

$正则化（regularization）：现在更多趋向于L2正则化

L2正则化：J(w,b)=(1/m)*sum(ɻ(yhat,y))+(人/2m)||w||^2---------------------------------这只是一个参数的例子，可以加b啥的。其中||w||=w.T*w，其中（人）是正则化参数（属于超参数），在代码中由于lambda是一个保留字写成lambd就行了

L1:将||w||^2转成||w||

例子：dw更新成：dw=(原先)+(λ/m)*w

正则化减少过拟合：解释是λ大的化，W就会减少，Z就会被惩罚导致Z减少，使得激活函数在线性范围内。

$dropout正则化：（不会怎么通用，数据量不够会用）（成本函数不明确，每次都会进行丢弃不利于优化）

随机丢弃一些单元，将他的激活后输出变成0，从而在反向传播中不会进行梯度更新，参数不会改变，不是参数（像是w）变成0，只是不参与反向传播的梯度更新

```python
def dropout_m(X,keep_prob):
    dropout_mask = np.random.rand(X.shape[0],X.shape[1]) < keep_prob        #生成随机向量，当小于保留概率的时候就是0，也可以直接使用*X.shape，其中*是解包操作符
    a=np.multiply(X,dropout_mask)                                           #将单元进行丢弃
    #a/= keep_prob                                                           #缩放回原来的大小，本质这个a其实就是输入X的dropout版本，这个代码不需要缩放，因为随机数组是真假形式
    return a
```

$标准化输入（归一化？）：

假设训练集二维，每个维的值做平均，再将每个相关维度的值除这个值得到标准化输入。

缩放（一般使用方差）：将标准化后的数据（上一步）进行缩放，就是计算这个维度所有值的平方的平均值，再使用标准化后的值除这个平均值。

归一化的好处：加入数据在平面上很挤，不利于学习，当缩放后数据就会分散在更大的平面，使得成本函数更加对称（而不是很窄），减少震荡来找到成本函数最小值

$梯度消失和爆炸：导数和斜率比较小，导致训练困难，随机权重初始化减少问题

例子：假设激活函数后值等于本身在最后一层Y=WWWWWWWX，在L很大的时候，假设W大于1，就会出现指数型增长，Y就会爆炸，小于1就会变得很小，Y就会指数级减少

$权重初始化：

一般做法：随机高斯变量乘以上一个层的单元个数(n)的平方根，这样的W不会比1太多和小太多，有效防止爆炸和消失

```python
W = np.random.randn(*X.shape)*np.sqrt(1/n)          #RELU一般是2/n
```

$梯度检验（确保反向传播正确）：

梯度数值的逼近：在导数中使用双边差分会更加逼近这个点的导数，如果使用单边（上三角/下三角）而不是双边的大三角形他的误差会更大（f(x+θ)-f(x-θ)）/2θ

梯度检验：首先将所有的参数重新构造成一个大向量θ，然后将他们的导也重塑一个大向量dθ，计算θ向量的导函数（上面的公式），检查是不是和dθ大致（使用距离：他们差的欧几里得距离的平方除以他们两个分别的欧几里得距离平方的和，结果小于10的-7次方----差不多）

注：1）不用在训练的时候使用，在debug使用检查错误。2）如果出现问题检查相关的组件找到bug（比如dw相差很近，但是db有问题可能是db计算错误有关b的错误）。3）正则化有也要加。4）不要和dropout一起。5）在初始化训练的时候进行，可能再次在训练中某个时间点进行，但是不要一直进行会降低训练速度

$mini-batch梯度下降法：

小批量梯度下降：虽然向量化可以加快计算，但是当数据量很大的时候（几百万级别），将整个输入的大矩阵输入计算依旧很慢

小批量梯度下降：将大数据分成小样本批次：X^{t},Y^{t}

小批量梯度下降训练中成本函数问题：会出现上升下降的震荡趋势，但是总体应该还是下降，因为每次训练都是训练新的小批次。

选择mini批次尺度：1就是随机了，可能会走错方向。

$几种比梯度下降更快的优化算法，下面是相关知识。

$指数加权平均：(在W和b上使用)

V(t)=βV(t-1)+(1-β)θ(t)------------------------其中V(t)是当前前的加权平均值，θ(t)是当前数据，那么可以计算1/(1-β)相当于这几个数据的加权平均，这范围内就会更加平滑

$动量梯度下降法：指数加权平均在W和b上使用

公式：(有缺口)




