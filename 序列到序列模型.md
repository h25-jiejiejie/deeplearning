```python
基础模型：
  构建一个编码器（encode可以是RNN，GRU或者LSTM等），最后编码器输出一个向量代表句子
  后面接一个解码器，直到最终输出（可能将解码器前一个yhat作为下一个组件的x输入）

图像生成标题：先进入卷积，后卷积的输出到序列模型中进行训练（网络可以通过自己的目的进行合理的组合）
```

```python
定向搜索（Beam serch）：
   参数B：尝试的可能（假如在语言翻译中，尝试第一个词是谁的可能B=3，可能尝试3种最大可能的单词，后面搜索的是单词对的可能不是下一个单词自己的可能）

改进定向搜索：
  长度归一化：
    arg max sum(logP(y<t>|x,y<1>,...y<t-1>))                其中t从1到Ty

误差分析：
  
```

```python
注意力模型前瞻：（像是一个RNN输入句子，一个RNN处理句子，这就是注意力模型）
  注意力直观理解：人类在翻译的时候不是开口就来或者看完一整句，而是阅读一部分翻译一部分，因为长难句真的很难
  现在的问题：在编码器和解码器的工作中，对于短句来说很好，但是对于长句子性能就会下降，因为神经网络很难记住很长的句子，进行解析

注意力模型：
  
```

```python

```





















