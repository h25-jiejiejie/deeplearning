 神经网络逻辑回归可以看成Z=W.T*X+B
 激活：a=g(Z)

其中W是很多个列向量w组成，在每个隐藏层单元每个w都是自己

每个隐藏层的隐藏单元分成两步：1）计算Z。2）计算激活后的a(simo/RELU啥的)

每一隐藏层的隐藏单元的w的维度都是由上一层的输出决定的

sigmoid事实上比较差，但是如果y∈{0，1}，那sigmoid函数会好（一般在就希望y^大于0小于1的时候使用），一般不知道使用什么作为激活函数时选择RELU

神经网络训练的初始的时候随机初始化权重很重要

参数更新的公式（用w来举例）：w=w-αdw     (其中α表示学习率，dw是梯度下降的倒数)

```python
np.sum(x,axis=0,keepdims=true)
#其中x就是需要求和的矩阵，axis=0表示竖直方向上求和（=1是垂直方向），keepdims表示保留这个样子，不会讲矩阵输出生为数组形式
#如果np.shape输出为（n, ）这种形式表示这是个数组不是矩阵
```

```python
A = np.zero(x,y) #初始化A矩阵全部是0
B = np.one（x,y）#初始化B矩阵全部是1
A = A.reshape(x1,y1) #重塑矩阵的状态，注意的是重塑的矩阵的元素数量不变才行
A.T #这个是矩阵的转置
```

如果不将权重随机初始化一个小值，一个层中的所有单元都会做一样的事情，这样就没有意义了

并且随机初始化W的时候需要一个小权重就行这个0.01只是一个例子；因为如果随机初始化权重很大的话在做像是二分类这样的问题的时候过激活函数Z也就是a会很大，他的导数就会接近0，遮藏梯度下降就会很慢（RELU不会有这种问题）
```python
W=np.random.randn(x,y)*0.01 #随机初始话权重W，很小的随机数
B=np.zeros(x,y) #B无所谓，不会影响他的平衡性
```


